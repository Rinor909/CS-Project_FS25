{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zurich Real Estate Price Prediction - Model Development\n",
    "\n",
    "This notebook focuses on building and evaluating machine learning models to predict real estate prices in Zurich based on property characteristics and travel time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "We'll load the processed datasets that were created during data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to load processed data files\n",
    "def load_processed_data():\n",
    "    processed_dir = os.path.join('..', 'data', 'processed')\n",
    "    \n",
    "    # Check if processed files exist\n",
    "    neighborhood_path = os.path.join(processed_dir, 'processed_neighborhood_data.csv')\n",
    "    building_age_path = os.path.join(processed_dir, 'processed_building_age_data.csv')\n",
    "    travel_times_path = os.path.join(processed_dir, 'travel_times.json')\n",
    "    \n",
    "    if not os.path.exists(neighborhood_path) or not os.path.exists(building_age_path):\n",
    "        print(\"Processed data files not found. Please run data_preparation.py first.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load the data\n",
    "    neighborhood_df = pd.read_csv(neighborhood_path)\n",
    "    building_age_df = pd.read_csv(building_age_path)\n",
    "    \n",
    "    # Check if travel times file exists\n",
    "    if os.path.exists(travel_times_path):\n",
    "        with open(travel_times_path, 'r') as f:\n",
    "            travel_times = json.load(f)\n",
    "    else:\n",
    "        print(\"Travel times data not found. Using None for now.\")\n",
    "        travel_times = None\n",
    "    \n",
    "    return neighborhood_df, building_age_df, travel_times\n",
    "\n",
    "# Load data\n",
    "neighborhood_df, building_age_df, travel_times = load_processed_data()\n",
    "\n",
    "if neighborhood_df is not None:\n",
    "    print(f\"Loaded {len(neighborhood_df)} neighborhood records\")\n",
    "    print(f\"Loaded {len(building_age_df)} building age records\")\n",
    "    print(f\"Travel times data available: {travel_times is not None}\")\n",
    "    \n",
    "    # Display the first few rows of each dataset\n",
    "    neighborhood_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "building_age_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling\n",
    "\n",
    "Let's prepare our feature set for modeling by combining neighborhood and building age data, and optionally adding travel time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_model_data(neighborhood_df, building_age_df, travel_times=None, latest_year_only=True):\n",
    "    \"\"\"Prepare data for model training\"\"\"\n",
    "    if neighborhood_df is None or building_age_df is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Filter for the latest year if requested\n",
    "    if latest_year_only:\n",
    "        latest_year = max(neighborhood_df['year'].max(), building_age_df['year'].max())\n",
    "        n_df = neighborhood_df[neighborhood_df['year'] == latest_year].copy()\n",
    "        ba_df = building_age_df[building_age_df['year'] == latest_year].copy()\n",
    "    else:\n",
    "        n_df = neighborhood_df.copy()\n",
    "        ba_df = building_age_df.copy()\n",
    "    \n",
    "    # For now, we'll use the neighborhood data as our primary dataset\n",
    "    # In a real implementation, we would join these datasets properly\n",
    "    X = n_df[['neighborhood', 'room_count', 'year']]\n",
    "    y = n_df['median_price']\n",
    "    \n",
    "    # If travel times are available, we could add them as features\n",
    "    # This is a placeholder for demonstration\n",
    "    if travel_times is not None:\n",
    "        print(\"Adding travel time features...\")\n",
    "        \n",
    "        # Example: Add average travel time to key destinations as a feature\n",
    "        # In a real implementation, this would be done more carefully\n",
    "        X['avg_travel_time'] = X['neighborhood'].apply(\n",
    "            lambda x: np.mean(list(travel_times.get(x, {}).values())) if x in travel_times else np.nan\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with the mean\n",
    "        X['avg_travel_time'].fillna(X['avg_travel_time'].mean(), inplace=True)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare data for modeling\n",
    "X, y = prepare_model_data(neighborhood_df, building_age_df, travel_times, latest_year_only=True)\n",
    "\n",
    "if X is not None:\n",
    "    print(f\"Prepared {len(X)} samples for modeling\")\n",
    "    print(f\"Feature columns: {X.columns.tolist()}\")\n",
    "    X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Let's split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Testing set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocessing Pipeline\n",
    "\n",
    "We'll create a preprocessing pipeline to handle categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_preprocessor(X):\n",
    "    \"\"\"Create a preprocessing pipeline for model features\"\"\"\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessing for categorical features (one-hot encoding)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Create preprocessing for numerical features (scaling)\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numerical_transformer, numerical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "if X is not None and len(X) > 0:\n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(X)\n",
    "    print(\"Preprocessing pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Random Forest\n",
    "\n",
    "Let's start with a Random Forest regression model as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Create pipeline with preprocessing and Random Forest\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    y_train_pred = rf_pipeline.predict(X_train)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    print(f\"Training set evaluation:\")\n",
    "    print(f\"MAE: {train_mae:.2f} CHF\")\n",
    "    print(f\"RMSE: {train_rmse:.2f} CHF\")\n",
    "    print(f\"RÂ²: {train_r2:.4f}\")\n",
    "    \n",
    "    # Evaluate on testing data\n",
    "    y_test_pred = rf_pipeline.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"\\nTesting set evaluation:\")\n",
    "    print(f\"MAE: {test_mae:.2f} CHF\")\n",
    "    print(f\"RMSE: {test_rmse:.2f} CHF\")\n",
    "    print(f\"RÂ²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Model: Gradient Boosting\n",
    "\n",
    "Let's try a Gradient Boosting model and compare its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Create pipeline with preprocessing and Gradient Boosting\n",
    "    gb_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training Gradient Boosting model...\")\n",
    "    gb_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on testing data\n",
    "    y_test_pred_gb = gb_pipeline.predict(X_test)\n",
    "    test_mae_gb = mean_absolute_error(y_test, y_test_pred_gb)\n",
    "    test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_test_pred_gb))\n",
    "    test_r2_gb = r2_score(y_test, y_test_pred_gb)\n",
    "    \n",
    "    print(f\"Testing set evaluation:\")\n",
    "    print(f\"MAE: {test_mae_gb:.2f} CHF\")\n",
    "    print(f\"RMSE: {test_rmse_gb:.2f} CHF\")\n",
    "    print(f\"RÂ²: {test_r2_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's examine which features are most important for predicting property prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_feature_importances(pipeline, X):\n",
    "    \"\"\"Plot feature importances from a trained Pipeline with a tree-based model\"\"\"\n",
    "    if hasattr(pipeline[-1], 'feature_importances_'):\n",
    "        # Get the preprocessor\n",
    "        preprocessor = pipeline.named_steps['preprocessor']\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Get categorical column names after one-hot encoding\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        if len(cat_cols) > 0:\n",
    "            ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "            cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "            feature_names.extend(cat_features)\n",
    "        \n",
    "        # Get numerical column names\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        feature_names.extend(num_cols)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = pipeline[-1].feature_importances_\n",
    "        \n",
    "        # Create a DataFrame for easier plotting\n",
    "        feature_importances = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot the top N features\n",
    "        top_n = min(20, len(feature_importances))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importances.head(top_n))\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importances\n",
    "    else:\n",
    "        print(\"Model does not have feature_importances_ attribute\")\n",
    "        return None\n",
    "\n",
    "if X is not None and len(X) > 0:\n",
    "    # Plot feature importances for Random Forest\n",
    "    print(\"Random Forest Feature Importances:\")\n",
    "    rf_feature_importances = plot_feature_importances(rf_pipeline, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Plot feature importances for Gradient Boosting\n",
    "    print(\"Gradient Boosting Feature Importances:\")\n",
    "    gb_feature_importances = plot_feature_importances(gb_pipeline, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameter Tuning\n",
    "\n",
    "Let's use grid search to find the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0 and len(X) > 100:  # Only run if we have enough data\n",
    "    # Define parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [50, 100],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # Create grid search\n",
    "    print(\"Running Grid Search for Random Forest (this may take a while)...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        rf_pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best RMSE: {np.sqrt(-grid_search.best_score_):.2f} CHF\")\n",
    "    \n",
    "    # Create final model with best parameters\n",
    "    best_rf_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Final Model\n",
    "\n",
    "Let's save the best model for use in our Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_model(model, model_name='price_model'):\n",
    "    \"\"\"Save the trained model to disk\"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    models_dir = os.path.join('..', 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(models_dir, f'{model_name}.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model_path\n",
    "\n",
    "if X is not None and len(X) > 0:\n",
    "    # Choose the best model to save\n",
    "    if 'best_rf_model' in locals():\n",
    "        final_model = best_rf_model\n",
    "        print(\"Saving the best Random Forest model from grid search...\")\n",
    "    elif 'rf_pipeline' in locals() and 'gb_pipeline' in locals():\n",
    "        # Compare RF and GB models\n",
    "        if test_r2 >= test_r2_gb:\n",
    "            final_model = rf_pipeline\n",
    "            print(\"Random Forest performed better. Saving this model...\")\n",
    "        else:\n",
    "            final_model = gb_pipeline\n",
    "            print(\"Gradient Boosting performed better. Saving this model...\")\n",
    "    else:\n",
    "        final_model = rf_pipeline\n",
    "        print(\"Saving the Random Forest model...\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = save_model(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing\n",
    "\n",
    "Let's test our model with some sample inputs to make sure it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Create some sample inputs\n",
    "    neighborhoods = X['neighborhood'].unique()[:3]  # Take first 3 neighborhoods\n",
    "    room_counts = X['room_count'].unique()[:2]  # Take first 2 room counts\n",
    "    \n",
    "    print(\"Testing model predictions for sample inputs:\")\n",
    "    print(\"\\nNeighborhoods:\", neighborhoods)\n",
    "    print(\"Room counts:\", room_counts)\n",
    "    \n",
    "    for neighborhood in neighborhoods:\n",
    "        for room_count in room_counts:\n",
    "            # Create input features\n",
    "            sample_input = pd.DataFrame({\n",
    "                'neighborhood': [neighborhood],\n",
    "                'room_count': [room_count],\n",
    "                'year': [X['year'].iloc[0]]  # Use the same year as in the dataset\n",
    "            })\n",
    "            \n",
    "            # Add travel time if it was included in the model\n",
    "            if 'avg_travel_time' in X.columns:\n",
    "                avg_time = travel_times.get(neighborhood, {})\n",
    "                if avg_time:\n",
    "                    sample_input['avg_travel_time'] = np.mean(list(avg_time.values()))\n",
    "                else:\n",
    "                    sample_input['avg_travel_time'] = X['avg_travel_time'].mean()\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = final_model.predict(sample_input)[0]\n",
    "            \n",
    "            print(f\"\\nPrediction for {room_count} room property in {neighborhood}:\")\n",
    "            print(f\"Estimated price: {prediction:,.2f} CHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Performance Summary\n",
    "\n",
    "Let's summarize the performance of our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if X is not None and len(X) > 0:\n",
    "    # Print final model summary\n",
    "    print(\"Final Model Performance Summary:\")\n",
    "    \n",
    "    # Re-evaluate on test set\n",
    "    y_pred_final = final_model.predict(X_test)\n",
    "    final_mae = mean_absolute_error(y_test, y_pred_final)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "    final_r2 = r2_score(y_test, y_pred_final)\n",
    "    \n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"Mean Absolute Error: {final_mae:,.2f} CHF\")\n",
    "    print(f\"Root Mean Squared Error: {final_rmse:,.2f} CHF\")\n",
    "    print(f\"RÂ² Score: {final_r2:.4f}\")\n",
    "    \n",
    "    # Visualize actual vs predicted prices\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_test, y_pred_final, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Price (CHF)')\n",
    "    plt.ylabel('Predicted Price (CHF)')\n",
    "    plt.title('Actual vs Predicted Real Estate Prices')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've developed a machine learning model to predict real estate prices in Zurich based on property characteristics. Key findings:\n",
    "\n",
    "1. Model Performance: [To be filled after running]\n",
    "2. Key Features: [To be filled after running]\n",
    "3. Patterns by Neighborhood: [To be filled after running]\n",
    "4. Price Sensitivity to Room Count: [To be filled after running]\n",
    "\n",
    "The final model has been saved and can be used in our Streamlit application for interactive price prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
