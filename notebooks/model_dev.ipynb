{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development for Zurich Real Estate Price Prediction\n",
    "\n",
    "This notebook develops and evaluates machine learning models for predicting real estate prices in Zurich.\n",
    "\n",
    "**Datasets:**\n",
    "1. Processed Property Prices by Neighborhood\n",
    "2. Processed Property Prices by Building Age\n",
    "3. Travel Time Data\n",
    "\n",
    "**Owner:** Rinor (Primary), Matthieu (Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define file paths\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "NEIGHBORHOOD_DATA = \"processed_neighborhood.csv\"\n",
    "BUILDING_AGE_DATA = \"processed_building_age.csv\"\n",
    "TRAVEL_TIME_DATA = \"neighborhood_travel_times.csv\"\n",
    "\n",
    "# Load processed neighborhood data\n",
    "neighborhood_path = os.path.join(PROCESSED_DATA_DIR, NEIGHBORHOOD_DATA)\n",
    "try:\n",
    "    neighborhood_df = pd.read_csv(neighborhood_path)\n",
    "    print(f\"Loaded processed neighborhood data with {neighborhood_df.shape[0]} rows and {neighborhood_df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {neighborhood_path}. Please run data_preparation.py first.\")\n",
    "    # For notebook development, we'll create dummy data\n",
    "    neighborhood_df = pd.DataFrame({\n",
    "        'neighborhood': ['Kreis ' + str(i) for i in range(1, 13) for _ in range(4)],\n",
    "        'room_count': [1, 2, 3, 4] * 12,\n",
    "        'median_price': np.random.randint(500000, 3000000, 48),\n",
    "        'year': [2020] * 48\n",
    "    })\n",
    "\n",
    "# Load processed building age data\n",
    "building_age_path = os.path.join(PROCESSED_DATA_DIR, BUILDING_AGE_DATA)\n",
    "try:\n",
    "    building_age_df = pd.read_csv(building_age_path)\n",
    "    print(f\"Loaded processed building age data with {building_age_df.shape[0]} rows and {building_age_df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {building_age_path}. Please run data_preparation.py first.\")\n",
    "    # For notebook development, we'll create dummy data\n",
    "    building_age_df = pd.DataFrame({\n",
    "        'building_age': ['Before 1919', '1919-1945', '1946-1960', '1961-1970', '1971-1980', '1981-1990', '1991-2000', '2001-2010', 'After 2010'],\n",
    "        'room_count': [3] * 9,\n",
    "        'median_price': np.random.randint(500000, 3000000, 9),\n",
    "        'year': [2020] * 9\n",
    "    })\n",
    "\n",
    "# Load travel time data\n",
    "travel_time_path = os.path.join(PROCESSED_DATA_DIR, TRAVEL_TIME_DATA)\n",
    "try:\n",
    "    travel_time_df = pd.read_csv(travel_time_path)\n",
    "    print(f\"Loaded travel time data with {travel_time_df.shape[0]} rows and {travel_time_df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {travel_time_path}. Please run generate_travel_times.py first.\")\n",
    "    # For notebook development, we'll create dummy data\n",
    "    neighborhoods = ['Kreis ' + str(i) for i in range(1, 13)]\n",
    "    destinations = ['Hauptbahnhof', 'ETH_Zurich', 'Zurich_Airport', 'Bahnhofstrasse']\n",
    "    travel_time_df = pd.DataFrame({\n",
    "        'neighborhood': [n for n in neighborhoods for _ in range(len(destinations))],\n",
    "        'destination': destinations * len(neighborhoods),\n",
    "        'travel_time_minutes': np.random.randint(5, 60, len(neighborhoods) * len(destinations))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_model_data(neighborhood_df, building_age_df, travel_time_df):\n",
    "    \"\"\"\n",
    "    Prepare data for model training by merging datasets and creating features.\n",
    "    \n",
    "    This is a sample implementation using the dummy data structure.\n",
    "    Adapt this function based on your actual data structure.\n",
    "    \"\"\"\n",
    "    # Reshape travel time data to have one row per neighborhood with columns for each destination\n",
    "    travel_time_wide = travel_time_df.pivot(index='neighborhood', columns='destination', values='travel_time_minutes')\n",
    "    travel_time_wide = travel_time_wide.reset_index()\n",
    "    travel_time_wide.columns = ['neighborhood'] + [f'travel_time_{dest.lower()}' for dest in travel_time_wide.columns[1:]]\n",
    "    \n",
    "    # Merge neighborhood data with travel time data\n",
    "    df = neighborhood_df.merge(travel_time_wide, on='neighborhood', how='left')\n",
    "    \n",
    "    # Create a building age factor based on building_age_df\n",
    "    # This is a simplified approach - in a real implementation, you would have a more sophisticated mapping\n",
    "    age_price_factor = building_age_df.set_index('building_age')['median_price'].to_dict()\n",
    "    building_age_map = {\n",
    "        'Before 1919': 0.9,\n",
    "        '1919-1945': 0.85,\n",
    "        '1946-1960': 0.8,\n",
    "        '1961-1970': 0.75,\n",
    "        '1971-1980': 0.8,\n",
    "        '1981-1990': 0.85,\n",
    "        '1991-2000': 0.9,\n",
    "        '2001-2010': 1.0,\n",
    "        'After 2010': 1.1\n",
    "    }\n",
    "    \n",
    "    # Add a dummy building age column for demonstration\n",
    "    np.random.seed(42)\n",
    "    df['building_age_category'] = np.random.choice(\n",
    "        list(building_age_map.keys()),\n",
    "        size=len(df)\n",
    "    )\n",
    "    df['building_age_factor'] = df['building_age_category'].map(building_age_map)\n",
    "    \n",
    "    # Define features and target\n",
    "    X = df[['neighborhood', 'room_count', 'building_age_factor'] + \n",
    "          [col for col in df.columns if 'travel_time' in col]]\n",
    "    y = df['median_price']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare data for modeling\n",
    "X, y = prepare_model_data(neighborhood_df, building_age_df, travel_time_df)\n",
    "\n",
    "# Display the first few rows of the feature matrix\n",
    "print(\"\\nFeature matrix (first 5 rows):\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "numerical_cols = [col for col in X.columns if X[col].dtype != 'object']\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"
